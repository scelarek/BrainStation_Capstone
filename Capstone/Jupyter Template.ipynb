{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Statistics & Public Health 2: Data Analysis](#toc0_)\n",
    "\n",
    "Sam Celarek  \n",
    "Data Science   \n",
    "scelarek@gmail.com  \n",
    "\n",
    "June 4th, 2023\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[1. Introduction](#toc0_)\n",
    "\n",
    "In this project, we will perform a set of analyses on the relationship between different variables and the mosquito number, as well as the probability of finding West Nile Virus (WNV) at any particular time and location. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[1.1. Key Questions](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc3_'></a>[2. Setup and Data Collection](#toc0_)\n",
    "\n",
    "We will be utilizing the cleaned mosquito tracking data from the city of Chicago, Illinois, spanning from 2008 to 2019 provided [here](link_to_dataset). This section will include the necessary libraries and modules for the analysis, as well as the data preparation steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "# initialize styling params\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) #setting figure size\n",
    "\n",
    "# Load the data\n",
    "\n",
    "# Display the first few rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/samcelarek/Documents/Github/Covid/epidemiology.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m folder_holder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/samcelarek/Documents/Github/Covid/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39m# # time series data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m epid_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mfolder_holder\u001b[39m}\u001b[39;49;00m\u001b[39mepidemiology.csv\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# 1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hospitalizations_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfolder_holder\u001b[39m}\u001b[39;00m\u001b[39mhospitalizations.csv\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# 2\u001b[39;00m\n\u001b[0;32m      6\u001b[0m vac_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfolder_holder\u001b[39m}\u001b[39;00m\u001b[39mvaccinations.csv\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# 3\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/samcelarek/Documents/Github/Covid/epidemiology.csv'"
     ]
    }
   ],
   "source": [
    "folder_holder = '/Users/samcelarek/Documents/Github/Covid/'\n",
    "\n",
    "# # time series data\n",
    "epid_df = pd.read_csv(f'{folder_holder}epidemiology.csv') # 1\n",
    "hospitalizations_df = pd.read_csv(f'{folder_holder}hospitalizations.csv') # 2\n",
    "vac_df = pd.read_csv(f'{folder_holder}vaccinations.csv') # 3\n",
    "mobility_df = pd.read_csv(f'{folder_holder}mobility.csv') # 4\n",
    "gov_response_df = pd.read_csv(f'{folder_holder}oxford-government-response.csv') # 5\n",
    "weather_df = pd.read_csv(f'{folder_holder}weather.csv') # 6\n",
    "\n",
    "timeland_df = [epid_df, hospitalizations_df, mobility_df, vac_df, gov_response_df, weather_df]\n",
    "\n",
    "\n",
    "# # location dfs\n",
    "geography_df = pd.read_csv(f'{folder_holder}geography.csv') #1\n",
    "health_df = pd.read_csv(f'{folder_holder}health.csv') #2\n",
    "demographics_df = pd.read_csv(f'{folder_holder}demographics.csv') #3\n",
    "economics_df = pd.read_csv(f'{folder_holder}economy.csv') #4\n",
    "locationland_df = [geography_df, health_df, demographics_df, economics_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(df):\n",
    "    # Filter rows based on column: 'location_key'\n",
    "    df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
    "    \n",
    "    # Change column type to datetime64[ns] for column: 'date'\n",
    "    try:\n",
    "        df = df.astype({'date': 'datetime64[ns]'})\n",
    "    except:\n",
    "        pass\n",
    "    # Change column type to category for column: 'location_key'\n",
    "    df = df.astype({'location_key': 'category'})\n",
    "    return df\n",
    "\n",
    "\n",
    "time_series_dfs = list(map(clean_data, timeland_df))\n",
    "\n",
    "# Assume dfs is your list of dataframes\n",
    "time_series_dfs = reduce(lambda left,right: pd.merge(left,right,on=['location_key', 'date'], how='left'), time_series_dfs).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_series_dfs.to_pickle('time_series_dfs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_series_dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m time_series_dfs\u001b[39m.\u001b[39msample(\u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_series_dfs' is not defined"
     ]
    }
   ],
   "source": [
    "time_series_dfs.sample(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here is a list of 10 countries that are often considered bellwethers for their regions and have good data collection, along with their 2-digit ISO country codes:\n",
    "\n",
    "United States (US)  \n",
    "Canada (CA)  \n",
    "Germany (DE)  \n",
    "United Kingdom (GB)  \n",
    "France (FR)  \n",
    "Japan (JP)  \n",
    "Australia (AU)  \n",
    "Brazil (BR)  \n",
    "South Africa (ZA)  \n",
    "India (IN)  \n",
    "These countries are often used as indicators for their respective regions due to their significant economic influence, political stability, and comprehensive data collection practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n",
      "/var/folders/1h/85xzk8r97_g7dvqy0gk21fsw0000gn/T/ipykernel_16234/2871392548.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df['location_key'].str.contains(r\"^(US|CA|DE|GB|FR|JP|AU|BR|ZA|IN)$\", na=False)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_key</th>\n",
       "      <th>openstreetmap_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>area_sq_km</th>\n",
       "      <th>area_rural_sq_km</th>\n",
       "      <th>area_urban_sq_km</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>smoking_prevalence</th>\n",
       "      <th>...</th>\n",
       "      <th>population_age_20_29</th>\n",
       "      <th>population_age_30_39</th>\n",
       "      <th>population_age_40_49</th>\n",
       "      <th>population_age_50_59</th>\n",
       "      <th>population_age_60_69</th>\n",
       "      <th>population_age_70_79</th>\n",
       "      <th>population_age_80_and_older</th>\n",
       "      <th>gdp_usd</th>\n",
       "      <th>gdp_per_capita_usd</th>\n",
       "      <th>human_capital_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GB</td>\n",
       "      <td>62149.0</td>\n",
       "      <td>54.6</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243610.0</td>\n",
       "      <td>183648.0</td>\n",
       "      <td>58698.0</td>\n",
       "      <td>81.30000</td>\n",
       "      <td>22.3</td>\n",
       "      <td>...</td>\n",
       "      <td>8697520.0</td>\n",
       "      <td>8872959.0</td>\n",
       "      <td>8465444.0</td>\n",
       "      <td>9020629.0</td>\n",
       "      <td>7101044.0</td>\n",
       "      <td>5567890.0</td>\n",
       "      <td>3319956.0</td>\n",
       "      <td>2.860009e+12</td>\n",
       "      <td>43070.0</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>1428125.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9984670.0</td>\n",
       "      <td>9084045.0</td>\n",
       "      <td>126511.0</td>\n",
       "      <td>81.94878</td>\n",
       "      <td>14.3</td>\n",
       "      <td>...</td>\n",
       "      <td>5096616.0</td>\n",
       "      <td>5278661.0</td>\n",
       "      <td>4846667.0</td>\n",
       "      <td>5182433.0</td>\n",
       "      <td>4712744.0</td>\n",
       "      <td>3018676.0</td>\n",
       "      <td>1664119.0</td>\n",
       "      <td>1.736426e+12</td>\n",
       "      <td>46194.0</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZA</td>\n",
       "      <td>87565.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>1219090.0</td>\n",
       "      <td>53460.0</td>\n",
       "      <td>53460.0</td>\n",
       "      <td>63.85700</td>\n",
       "      <td>20.3</td>\n",
       "      <td>...</td>\n",
       "      <td>10141489.0</td>\n",
       "      <td>10155325.0</td>\n",
       "      <td>7043275.0</td>\n",
       "      <td>4911532.0</td>\n",
       "      <td>3164441.0</td>\n",
       "      <td>1476055.0</td>\n",
       "      <td>421794.0</td>\n",
       "      <td>3.514316e+11</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>0.406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_key  openstreetmap_id  latitude  longitude  elevation_m  \\\n",
       "5           GB           62149.0      54.6       -2.0          NaN   \n",
       "2           CA         1428125.0      56.0     -109.0          NaN   \n",
       "9           ZA           87565.0     -29.0       24.0       1037.0   \n",
       "\n",
       "   area_sq_km  area_rural_sq_km  area_urban_sq_km  life_expectancy  \\\n",
       "5    243610.0          183648.0           58698.0         81.30000   \n",
       "2   9984670.0         9084045.0          126511.0         81.94878   \n",
       "9   1219090.0           53460.0           53460.0         63.85700   \n",
       "\n",
       "   smoking_prevalence  ...  population_age_20_29  population_age_30_39  \\\n",
       "5                22.3  ...             8697520.0             8872959.0   \n",
       "2                14.3  ...             5096616.0             5278661.0   \n",
       "9                20.3  ...            10141489.0            10155325.0   \n",
       "\n",
       "   population_age_40_49  population_age_50_59  population_age_60_69  \\\n",
       "5             8465444.0             9020629.0             7101044.0   \n",
       "2             4846667.0             5182433.0             4712744.0   \n",
       "9             7043275.0             4911532.0             3164441.0   \n",
       "\n",
       "   population_age_70_79  population_age_80_and_older       gdp_usd  \\\n",
       "5             5567890.0                    3319956.0  2.860009e+12   \n",
       "2             3018676.0                    1664119.0  1.736426e+12   \n",
       "9             1476055.0                     421794.0  3.514316e+11   \n",
       "\n",
       "   gdp_per_capita_usd  human_capital_index  \n",
       "5             43070.0                0.781  \n",
       "2             46194.0                0.799  \n",
       "9              6001.0                0.406  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "location_df = list(map(clean_data, locationland_df))\n",
    "\n",
    "# Assume dfs is your list of dataframes\n",
    "location_df = reduce(lambda left,right: pd.merge(left,right,on='location_key', how='left'), location_df).copy()\n",
    "\n",
    "location_df.sample(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Wireframe:\n",
    "\n",
    "1. Date and Location:\n",
    "    - 'date'\n",
    "    - 'location_key'\n",
    "\n",
    "2. COVID-19 Statistics:\n",
    "    - 'new_confirmed' (New Positive Cases)\n",
    "    - 'new_deceased' (New Deaths)\n",
    "    - 'new_hospitalized_patients' (New Hospitalizations)\n",
    "\n",
    "3. Mobility Data:\n",
    "    - 'mobility_retail_and_recreation'\n",
    "    - 'mobility_grocery_and_pharmacy'\n",
    "    - 'mobility_parks'\n",
    "    - 'mobility_transit_stations'\n",
    "    - 'mobility_workplaces'\n",
    "    - 'mobility_residential'\n",
    "\n",
    "4. Vaccination Data:\n",
    "    - 'new_persons_vaccinated'\n",
    "    - 'cumulative_persons_vaccinated'\n",
    "    - 'new_persons_fully_vaccinated'\n",
    "    - 'cumulative_persons_fully_vaccinated'\n",
    "    - 'new_vaccine_doses_administered'\n",
    "    - 'cumulative_vaccine_doses_administered'\n",
    "\n",
    "5. Policy Measures:\n",
    "    - 'school_closing'\n",
    "    - 'workplace_closing'\n",
    "    - 'cancel_public_events'\n",
    "    - 'restrictions_on_gatherings'\n",
    "    - 'public_transport_closing'\n",
    "    - 'stay_at_home_requirements'\n",
    "    - 'restrictions_on_internal_movement'\n",
    "    - 'international_travel_controls'\n",
    "    - 'income_support'\n",
    "    - 'debt_relief'\n",
    "    - 'fiscal_measures'\n",
    "    - 'international_support'\n",
    "    - 'public_information_campaigns'\n",
    "    - 'testing_policy'\n",
    "    - 'contact_tracing'\n",
    "    - 'emergency_investment_in_healthcare'\n",
    "    - 'investment_in_vaccines'\n",
    "    - 'facial_coverings'\n",
    "    - 'vaccination_policy'\n",
    "    - 'stringency_index'\n",
    "\n",
    "6. Weather Data:\n",
    "    - 'average_temperature_celsius'\n",
    "    - 'minimum_temperature_celsius'\n",
    "    - 'maximum_temperature_celsius'\n",
    "    - 'rainfall_mm'\n",
    "    - 'snowfall_mm'\n",
    "    - 'dew_point'\n",
    "    - 'relative_humidity'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe provides a comprehensive snapshot of COVID-19 data, mobility metrics, government restrictions, and weather conditions for specific locations on specific dates. Here's a brief overview of the columns:\n",
    "\n",
    "1. `Entry ID`: A unique identifier for each row in the dataframe.\n",
    "2. `Date`: The date for the day on which the data was recorded.\n",
    "3. `Location Key`: A code representing the location (10 different countries in total) for which the data is reported.\n",
    "\n",
    "4. `New Confirmed`: The number of new confirmed COVID-19 cases on the given date.\n",
    "5. `New Deceased`: The number of new COVID-19 related deaths on the given date.\n",
    "6. `New Recovered`: The number of new recoveries from COVID-19 on the given date.\n",
    "7. `New Tested`: The number of new COVID-19 tests conducted on the given date.\n",
    "\n",
    "8. `New Hospitalizations`: The number of new hospitalizations due to COVID-19 on the given date.\n",
    "9. `Current Hospitalizations`: The total number of current hospitalizations due to COVID-19 on the given date.\n",
    "\n",
    "10. `New Fully Vaccinated (29+ other Vaccination Columns)`: The number of new fully vaccinated individuals on the given date. There are 29 other columns related to vaccination data here too.\n",
    "\n",
    "11. `Retail and Recreation Mobility (5+ other Mobility Metrics)`: A measure of mobility in retail and recreation spaces, along with 5 other columns related to different aspects of mobility.\n",
    "\n",
    "12. `School Closing (19+ other Government Restrictions)`: A measure indicating whether schools were closed on the given date, along with 19 other columns related to different government restrictions.\n",
    "\n",
    "13. `Average Temp (6+ Other Weather Columns)`: The average temperature on the given date, along with 6 other columns related to different weather conditions.\n",
    "\n",
    "In total there are 9880 and 82 rows for 6.3mbs of data. The main way I could increase or decrease the size of the dataset would be to include more countries, regions, or counties in the analysis. For now this is my starter df.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
